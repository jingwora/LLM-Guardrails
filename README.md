# LLM-Guardrails

This collection of tools provides robust safety and compliance measures for large language models (LLMs). Each tool serves a unique purpose in enhancing the reliability and security of LLM applications.


| Framework/Tool                      | Purpose                                           | Key Features                                                  | GitHub Stars                          | Link                                |
|-------------------------------------|---------------------------------------------------|---------------------------------------------------------------|---------------------------------------|-------------------------------------|
| NeMo Guardrails                     | Adds programmable guardrails to LLM applications   | Topical, safety, and security guardrails; supports multiple LLMs | 3.6k stars  | [source](https://github.com/NVIDIA/NeMo-Guardrails) |
| Guardrails AI                       | Ensures safe, reliable, and compliant behavior of LLMs | Rule-based safeguards, real-time monitoring, policy enforcement | 3.5k stars  | [source](https://github.com/guardrails-ai/guardrails) |
| PyRIT (Microsoft's Red Teaming)     | Automates risk identification in generative AI systems | Malicious prompt generation, risk scoring, iterative attack strategies | 1.4k stars   | [source](https://github.com/Azure/PyRIT)    |
| OpenAI Moderation                   | Filters harmful or inappropriate content          | Predefined content filters, API integration                    | Not open source  | [source]([https://openai.com/api/moderation](https://platform.openai.com/docs/guides/moderation)                        |
| Azure AI Content Safety                   | Enhances content safety and compliance checks          | Safeguard user and AI-generated text and image content.                  | Not open source  | [source](https://azure.microsoft.com/ja-jp/products/ai-services/ai-content-safety)                        |
